{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from example_config import SECRET_KEY\n",
    "openai.api_key = SECRET_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the query parameters\n",
    "query = 'generative adversarial network'\n",
    "results = 25\n",
    "sort_by = arxiv.SortCriterion.Relevance\n",
    "sort_order = arxiv.SortOrder.Ascending\n",
    "\n",
    "papers = arxiv.Search(query=query, max_results=results, sort_by=sort_by, sort_order=sort_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = papers.results() # now we can iterate over the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Network Robustness with Adversary Critic\n",
      "Adversarial symmetric GANs: bridging adversarial samples and adversarial networks\n",
      "ASAT: Adaptively Scaled Adversarial Training in Time Series\n",
      "Improving Global Adversarial Robustness Generalization With Adversarially Trained GAN\n",
      "Strength-Adaptive Adversarial Training\n",
      "On the Effect of Adversarial Training Against Invariance-based Adversarial Examples\n",
      "Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to DNNs\n",
      "Beneficial Perturbations Network for Defending Adversarial Examples\n",
      "Generative adversarial networks and adversarial methods in biomedical image analysis\n",
      "Finding Dynamics Preserving Adversarial Winning Tickets\n",
      "Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks\n",
      "Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN\n",
      "Latent Adversarial Defence with Boundary-guided Generation\n",
      "LADDER: Latent Boundary-guided Adversarial Training\n",
      "Synthesizing Unrestricted False Positive Adversarial Objects Using Generative Models\n",
      "A Direct Approach to Robust Deep Learning Using Adversarial Networks\n",
      "Imperceptible Adversarial Attack via Invertible Neural Networks\n",
      "Lung image segmentation by generative adversarial networks\n",
      "Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization\n",
      "Perlin Noise Improve Adversarial Robustness\n",
      "Robustness through Cognitive Dissociation Mitigation in Contrastive Adversarial Training\n",
      "Semantic Segmentation using Adversarial Networks\n",
      "Adversarial Diversity and Hard Positive Generation\n",
      "Adversarial Transformation Networks: Learning to Generate Adversarial Examples\n",
      "Delving into Transferable Adversarial Examples and Black-box Attacks\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "summaries_df = pd.DataFrame()\n",
    "\n",
    "for paper in papers:\n",
    "    print(paper.title)\n",
    "    # print(paper.summary) # this is the summary of the paper\n",
    "    # print(paper.pdf_url) # this is the url of the pdf\n",
    "\n",
    "    summaries_df = summaries_df.append({'content': paper.summary}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledgegpt.extractors.base_extractor import BaseExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_extractor = BaseExtractor(dataframe=summaries_df,embedding_extractor=\"hf\", model_lang=\"en\", is_turbo=True, index_path=None, index_type=\"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings...\n",
      "model_lang en\n",
      "Selected 3 document sections:\n",
      "0\n",
      "4\n",
      "9\n",
      "Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\n",
      "\n",
      "Context:\n",
      "\n",
      "* Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing. Codes for the project are available at https://github.com/aam-at/adversary_critic.\n",
      "* Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose \\emph{Strength-Adaptive Adversarial Training} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training.\n",
      "* Modern deep neural networks (DNNs) are vulnerable to adversarial attacks and adversarial training has been shown to be a promising method for improving the adversarial robustness of DNNs. Pruning methods have been considered in adversarial context to reduce model capacity and improve adversarial robustness simultaneously in training. Existing adversarial pruning methods generally mimic the classical pruning methods for natural training, which follow the three-stage 'training-pruning-fine-tuning' pipelines. We observe that such pruning methods do not necessarily preserve the dynamics of dense networks, making it potentially hard to be fine-tuned to compensate the accuracy degradation in pruning. Based on recent works of \\textit{Neural Tangent Kernel} (NTK), we systematically study the dynamics of adversarial training and prove the existence of trainable sparse sub-network at initialization which can be trained to be adversarial robust from scratch. This theoretically verifies the \\textit{lottery ticket hypothesis} in adversarial context and we refer such sub-network structure as \\textit{Adversarial Winning Ticket} (AWT). We also show empirical evidences that AWT preserves the dynamics of adversarial training and achieve equal performance as dense adversarial training.\n",
      "\n",
      " Q: Improved Network Robustness with Adversary Critic, tell me about it. \n",
      " A:\n",
      "all_done!\n"
     ]
    }
   ],
   "source": [
    "answer, prompt, messages = base_extractor.extract(\"Improved Network Robustness with Adversary Critic, tell me about it. \", max_tokens=1000, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The article proposes a novel approach for learning a robust classifier using Generative Adversarial Networks (GAN). The adversarial attack on the classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, they introduce an adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. The experiments show the effectiveness of their defense, which surpasses networks trained with adversarial training. Additionally, they verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don\\'t know.\"\\n\\nContext:\\n\\n* Ideally, what confuses neural network should be confusing to humans. However, recent experiments have shown that small, imperceptible perturbations can change the network prediction. To address this gap in perception, we propose a novel approach for learning robust classifier. Our main idea is: adversarial examples for the robust classifier should be indistinguishable from the regular data of the adversarial target. We formulate a problem of learning robust classifier in the framework of Generative Adversarial Networks (GAN), where the adversarial attack on classifier acts as a generator, and the critic network learns to distinguish between regular and adversarial images. The classifier cost is augmented with the objective that its adversarial examples should confuse the adversary critic. To improve the stability of the adversarial mapping, we introduce adversarial cycle-consistency constraint which ensures that the adversarial mapping of the adversarial examples is close to the original. In the experiments, we show the effectiveness of our defense. Our method surpasses in terms of robustness networks trained with adversarial training. Additionally, we verify in the experiments with human annotators on MTurk that adversarial examples are indeed visually confusing. Codes for the project are available at https://github.com/aam-at/adversary_critic.\\n* Adversarial training (AT) is proved to reliably improve network\\'s robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network\\'s desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose \\\\emph{Strength-Adaptive Adversarial Training} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training.\\n* Modern deep neural networks (DNNs) are vulnerable to adversarial attacks and adversarial training has been shown to be a promising method for improving the adversarial robustness of DNNs. Pruning methods have been considered in adversarial context to reduce model capacity and improve adversarial robustness simultaneously in training. Existing adversarial pruning methods generally mimic the classical pruning methods for natural training, which follow the three-stage \\'training-pruning-fine-tuning\\' pipelines. We observe that such pruning methods do not necessarily preserve the dynamics of dense networks, making it potentially hard to be fine-tuned to compensate the accuracy degradation in pruning. Based on recent works of \\\\textit{Neural Tangent Kernel} (NTK), we systematically study the dynamics of adversarial training and prove the existence of trainable sparse sub-network at initialization which can be trained to be adversarial robust from scratch. This theoretically verifies the \\\\textit{lottery ticket hypothesis} in adversarial context and we refer such sub-network structure as \\\\textit{Adversarial Winning Ticket} (AWT). We also show empirical evidences that AWT preserves the dynamics of adversarial training and achieve equal performance as dense adversarial training.\\n\\n Q: Improved Network Robustness with Adversary Critic, tell me about it. \\n A:'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to download the pdfs and then make q&a with them you can use the following code\n",
    "# then you can use the pdfs as the content for the knowledgegpt\n",
    "\n",
    "# import urllib.request # this is only needed if you want to download the pdfs\n",
    "\n",
    "# for paper in papers:\n",
    "    \n",
    "#     # Download the PDF\n",
    "#     pdf_filename = paper.title.replace(\" \", \"_\") + \".pdf\"\n",
    "#     urllib.request.urlretrieve(paper.pdf_url, pdf_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledgegpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
